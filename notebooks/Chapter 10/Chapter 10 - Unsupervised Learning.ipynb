{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto no aprendizado supervisionado estamos preocupados na predição de nossa variável de resposta Y, com base em nossas variáveis independentes X1, X2 ... Xp. No aprendizado não supervisionado estamos interessados em descobrir coisas interessantes sobre medições em X1, X2 ... Xp. \n",
    "\n",
    "Respondemos perguntas como:\n",
    "\n",
    "- Existe uma maneira informativa de visualizar os dados?\n",
    "\n",
    "- Descobrimos subgrupos entre as variáveis ou entre as observações?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 The Challenge of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No aprendizado supervisionado, há uma noção clara do que estamos buscando como saída do modelo (a nossa predição de Y). Há métricas para acompanharmos o quanto nosso modelo performa.\n",
    "\n",
    "No aprendizado não supervisionado costuma ser mais subjetivo e não há um objetivo simples para a análise como a predição de uma resposta. \n",
    "\n",
    "- É frequentemente utilizado em análise exploratória;\n",
    "- Dificil avaliar os resultados obtidos. Não existe um mecanismo universalmente aceito para executar a validação cruzada\n",
    "  ou validar em um conjunto de dados independentes.\n",
    "- Não sabemos a resposta verdadeira, não é um problema supervionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os componentes principais nos permite resumir um conjunto de variáveis correlacionadas em um número menor de variáveis representativas que explicam coletivamente a maior parte da nossa variabilidade no conjunto original.\n",
    "\n",
    "- Produz variáveis derivadas para uso em problemas supervisionados;\n",
    "- Serve como uma ferramenta para a visualização de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 What Are Principal Components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ele encontra uma representação de baixa dimensão de um conjunto de dados que contém o máximo possível da variação.\n",
    "\n",
    "A idéia é que cada uma das n observações viva no espaço p-dimensional. \n",
    "\n",
    "PCA procura um pequeno número e dimensões tão interessantes quanto possível, onde o conceito de interessante é medido pela quantidade em que as observações variam ao longo de cada dimensão. \n",
    "\n",
    "Cada uma das dimensões encontradas pelo PCA é uma combinação linear dos recursos p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PCA matemáticamente definino como uma transformação ortogonal que transforma os dados para um novo sistema de coordenadas de forma que a variância por qualquer projeção dos dados fica ao longo da primeira coordenada (First Princical Component), a segunda maior variância fica ao longo da segunda coordenada, e assim por diante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Otimização__\n",
    "\n",
    "![](figure/figure10.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/pca_gif.gif) ![](figure/pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Visão animada: https://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- os vetores de carregamento do componente principal como as direções no espaço de recursos ao longo das quais os dados variam mais\n",
    "\n",
    "- o componente principal pontua como projeções nessas direções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Another Interpretation of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esquerda: as duas primeiras direções dos componentes principais abrangem o plano que melhor se ajusta aos dados. Isto\n",
    "minimiza a soma das distâncias ao quadrado de cada ponto ao plano. \n",
    "\n",
    "Direita:Os dois primeiros vetores de pontuação dos componentes principais fornecem as coordenadas da projeção das 90 observações. A variação no plano é maximizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Proportion of Variance Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding How Many Principal Components to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos componentes principais são necessários? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar o metódo do cotovelo para encontrar o número ideal de componentes principais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma abordagem simples de particioar um conjunto de dados em K clusters distintos e sem sobreposição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premissas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Cada observação pertence a pelo menos um dos clusters K\n",
    "\n",
    "2 - Os clusters não se sobrepõem: nenhuma observação pertence a mais de um cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia por trás do agrupamento K-means é que um bom agrupamento é aquele para o qual o variação dentro do cluster é a menor possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variação dentro do cluster para o k-ésimo cluster é a soma de todas as distâncias euclidianas quadradas aos pares entre as observações no k-ésimo cluster, divididas pelo número total de observações no k-ésimo cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapas\n",
    "\n",
    "1. Inicializar os centroides aleatoriamente (centros do cluster).\n",
    "\n",
    "\n",
    "2. Iterar até que pare as atribuições aos clusters.\n",
    "\n",
    "    a) Para cada k cluster, computar o centróide do cluster (média das observações de k cluster).\n",
    "    \n",
    "    b) Atribua cada observação ao cluster cujo centróide está mais próximo (onde o mais próximo for definido usando a distância euclidiana)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/k_means.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma desvantegem do k-means é especificar a quantidade de clusters k. Agrupamento hierárquico é uma abordagem alternativa que não requer a necessidade de informar o número de clusters. Outra vantagem adicional sobre o K-means é uma representação atraente baseada em árvore das observações, chamada dendrograma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting a Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quarenta e cinco observações geradas no espaço bidimensional. Na realidade, existem três classes distintas, mostradas em cores separadas. No entanto, trataremos esses rótulos de classe como desconhecidos e procuraremos agrupar as observações para descobrir as classes a partir dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As fusões mais abaixo da árvore representam as observações mais similares. A medida que as folhas se fudem em galhos, as observações vão se diferenciando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/figure10.10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não dá para tirar conclusões sobre a similaridade de duas observações com base no eixo vertical. Em vez disso, tiramos conclusões sobre a semelhança de duas observações com base na localização no eixo vertical onde os ramos que contêm essas duas observações primeiro são fundidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A altura do corte no dendrograma serve o mesmo papel que o agrupamento K em K-means: controla o número de clusters obtidos.\n",
    "\n",
    "Os conjuntos distintos de observações abaixo do corte podem ser interpretados como aglomerados.\n",
    "\n",
    "O termo hierárquico refere-se ao fato de que os clusters obtidos pelo corte do dendrograma a uma determinada altura são necessariamente aninhados dentro dos clusters obtidos pelo corte do dendrograma a qualquer altura maior.\n",
    "\n",
    "Em um conjunto de dados arbitrário, essa suposição de estrutura hierárquica pode não ser realista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Hierarchical Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir o dendograma:\n",
    "\n",
    "1 - definimos algum tipo de medida de dissimilaridade entre cada par de observações (geralmente distância euclidiana);\n",
    "\n",
    "2 - o algoritmo prossegue iterativamente. Começando na parte inferior do dendrograma, cada uma das n observações é tratada como seu próprio cluster. Os dois clusters\n",
    "que são mais parecidos entre si são fundidos para que agora haja n-1 clusters.\n",
    "\n",
    "3 - em seguida, os dois clusters mais semelhantes entre si são fundido novamente, de modo que agora existem n - 2 clusters. O algoritmo continua\n",
    "dessa maneira, até que todas as observações pertençam a um único cluster e o dendograma está completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo conceito que usamos de dissimilaridade entre pares de observações, precisa ser levado para a visão de pares de cluster. Essa noção de dissimilaridade é conhecida como ligação. São quatros tipos comuns de ligação: completo, médio, único e centróide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figure/hierarch.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comece com n observações e uma medida (como a distância euclidiana) de todas as $\\binom{n}{2} = n (n -1)/ 2$ dissimilaridades em pares. Trate cada observação como seu próprio cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For $i = n, n − 1, . . . , 2$:\n",
    "\n",
    "    a) Examine todas as dissimilaridades entre pares inter-cluster entre os grupos i clusters e identifique o par de clusters menos diferente (ou seja, mais semelhante). Fundir esses dois clusters. A dissimilaridade entre esses dois grupos indica a altura no dendrograma na qual a fusão deve ser colocada.\n",
    "    \n",
    "    b) Calcular as novas diferenças pares inter-clusters entre os $i - 1$ clusters restantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Complete__ \n",
    "\n",
    "Dissimilaridade máxima do intercluster.\n",
    "\n",
    "![](figure/linkage_complete.png)\n",
    "\n",
    "__Single__ \n",
    "\n",
    "Dissimilaridade mínima do intercluster. Articulação única pode resultar em prolongamento clusters nos quais observações únicas são fundidas uma de cada vez.\n",
    "\n",
    "![](figure/linkage_single.png)\n",
    "__Average__ \n",
    "\n",
    "Dissimilaridade média do intercluster. \n",
    "![](figure/linkage_average.png)\n",
    "\n",
    "__Centroid__\n",
    "\n",
    "Dissimilaridade entre o centróide para o cluster A (uma média vetor de comprimento p) e o centróide para o cluster B. Centroid ligação pode resultar em inversões indesejáveis.\n",
    "\n",
    "![](figure/linkage_centroid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 Practical Issues in Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Decisions with Big Consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deveriámos padronizar os dados?\n",
    "\n",
    "No caso do agrupamento hierárquico,\n",
    "\n",
    "- Qual medida de dissimilaridade deveria ser usada?\n",
    "- Qual tipo de ligação deveria ser usada?\n",
    "- Onde devemos cortar o dendograma para obter aglomerados?\n",
    "\n",
    "No caso do agrupamento K-means, quantos clusters devemos procurar\n",
    "para os dados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise dos Componentes Principais \n",
    "\n",
    "https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais#:~:text=Dado%20um%20conjunto%20de%20pontos,dist%C3%A2ncias%20dos%20pontos%20%C3%A0%20linha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StatQuest: Principal Component Analysis (PCA), Step-by-Step \n",
    "\n",
    "https://www.youtube.com/watch?v=FgakZw6K1QQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering with Scikit with GIFs \n",
    "\n",
    "https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gentle Introduction to Eigenvalues and Eigenvectors for Machine Learning \n",
    "\n",
    "https://machinelearningmastery.com/introduction-to-eigendecomposition-eigenvalues-and-eigenvectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering 3: single-link vs. complete-link \n",
    "    \n",
    "https://www.youtube.com/watch?v=VMyXc3SiEqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essence of linear algebra \n",
    "\n",
    "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
